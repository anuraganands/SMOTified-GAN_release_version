{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/connect-4/c4_game_database.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/connect-4/c4_game_database.csv\", sep=\",\", header='infer' )\n","metadata":{"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"        pos_01  pos_02  pos_03  pos_04  pos_05  pos_06  pos_07  pos_08  \\\n0          1.0     1.0     1.0    -1.0    -1.0     1.0     0.0    -1.0   \n1          0.0     0.0     1.0     1.0     1.0     1.0     0.0     0.0   \n2          0.0     1.0     0.0     1.0     0.0     0.0     0.0     0.0   \n3          0.0     0.0     0.0     1.0     0.0     0.0     0.0     0.0   \n4          0.0    -1.0    -1.0    -1.0     1.0     0.0     0.0     1.0   \n...        ...     ...     ...     ...     ...     ...     ...     ...   \n376635     0.0     0.0     0.0    -1.0     0.0     0.0     0.0     0.0   \n376636     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n376637     0.0     0.0     1.0     0.0     0.0     1.0     0.0     0.0   \n376638     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n376639     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n\n        pos_09  pos_10  ...  pos_34  pos_35  pos_36  pos_37  pos_38  pos_39  \\\n0         -1.0    -1.0  ...     1.0     1.0    -1.0     1.0    -1.0     1.0   \n1          0.0     1.0  ...    -1.0     1.0     1.0    -1.0    -1.0    -1.0   \n2         -1.0     0.0  ...     0.0    -1.0     1.0    -1.0    -1.0     1.0   \n3          0.0     0.0  ...     0.0     1.0     1.0     1.0    -1.0     1.0   \n4          1.0     1.0  ...     0.0     1.0    -1.0    -1.0     1.0    -1.0   \n...        ...     ...  ...     ...     ...     ...     ...     ...     ...   \n376635     0.0    -1.0  ...     0.0     1.0    -1.0    -1.0    -1.0     1.0   \n376636     0.0    -1.0  ...     0.0     0.0     0.0    -1.0     1.0    -1.0   \n376637     0.0    -1.0  ...     1.0     1.0    -1.0    -1.0    -1.0     1.0   \n376638    -1.0     0.0  ...    -1.0     0.0    -1.0    -1.0     1.0     1.0   \n376639     0.0     0.0  ...    -1.0     0.0    -1.0    -1.0    -1.0     1.0   \n\n        pos_40  pos_41  pos_42  winner  \n0         -1.0     1.0    -1.0    -1.0  \n1          1.0     1.0    -1.0     1.0  \n2         -1.0    -1.0    -1.0    -1.0  \n3         -1.0     0.0     1.0    -1.0  \n4          1.0     0.0    -1.0     1.0  \n...        ...     ...     ...     ...  \n376635    -1.0     0.0     1.0    -1.0  \n376636     1.0     0.0     0.0    -1.0  \n376637     1.0     1.0    -1.0    -1.0  \n376638     1.0    -1.0     0.0    -1.0  \n376639    -1.0     1.0    -1.0     1.0  \n\n[376640 rows x 43 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>pos_01</th>\n      <th>pos_02</th>\n      <th>pos_03</th>\n      <th>pos_04</th>\n      <th>pos_05</th>\n      <th>pos_06</th>\n      <th>pos_07</th>\n      <th>pos_08</th>\n      <th>pos_09</th>\n      <th>pos_10</th>\n      <th>...</th>\n      <th>pos_34</th>\n      <th>pos_35</th>\n      <th>pos_36</th>\n      <th>pos_37</th>\n      <th>pos_38</th>\n      <th>pos_39</th>\n      <th>pos_40</th>\n      <th>pos_41</th>\n      <th>pos_42</th>\n      <th>winner</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>-1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-1.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>-1.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>-1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>376635</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-1.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>376636</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-1.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-1.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>376637</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-1.0</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>376638</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-1.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-1.0</td>\n      <td>0.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>0.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>376639</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-1.0</td>\n      <td>0.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>376640 rows × 43 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"X = data.values[:,0:42].astype(float)\nY = data.values[:,42]","metadata":{"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"preX = data[:,0:42]\npreY = data[:,42]\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfrom keras.utils import np_utils\n","metadata":{"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"encoder = LabelEncoder()\nencoder.fit(Y)\nencoded_Y = encoder.transform(Y)\ndummy_y = np_utils.to_categorical(encoded_Y)","metadata":{"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"ysi=pd.Series(encoded_Y) \nysi.value_counts()","metadata":{"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"2    181255\n0    180867\n1     14497\n3        21\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"yk=[]\nfor i in encoded_Y:\n    if i==1:\n        yk.append(1)\n    else:\n        yk.append(0)","metadata":{"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"ysi=pd.Series(yk) \nysi.value_counts()","metadata":{"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"0    362143\n1     14497\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"y = np.asarray(yk, dtype=np.float32)\ny.shape","metadata":{"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"(376640,)"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","metadata":{"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"print(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train==1)))\nprint(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train==0)))\nfrom imblearn.over_sampling import SMOTE\nX_train_res,y_train_res = SMOTE().fit_resample(X_train,y_train)\n\nprint('After OverSampling, the shape of train_X: {}'.format(X_train_res.shape))\nprint('After OverSampling, the shape of train_y: {} \\n'.format(y_train_res.shape))\n\nprint(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res==1)))\nprint(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res==0)))","metadata":{"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Before OverSampling, counts of label '1': 11562\nBefore OverSampling, counts of label '0': 289750 \n\nAfter OverSampling, the shape of train_X: (579500, 42)\nAfter OverSampling, the shape of train_y: (579500,) \n\nAfter OverSampling, counts of label '1': 289750\nAfter OverSampling, counts of label '0': 289750\n","output_type":"stream"}]},{"cell_type":"code","source":"# TensorFlow and tf.keras\nimport tensorflow as tf\n\n# Helper libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nprint(tf.__version__)\nfrom sklearn.metrics import f1_score\nfrom statistics import stdev","metadata":{"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"2.4.1\n","output_type":"stream"}]},{"cell_type":"code","source":"model = tf.keras.Sequential([\n    tf.keras.layers.Dense(256, activation='relu'),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dense(1)\n])\nmodel.compile(optimizer='adam',\n              loss='mean_absolute_error',\n              metrics=['accuracy'])\nmodel.fit(X_train_res,y_train_res.ravel() , epochs=3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_loss, test_acc = model.evaluate(X_test,  y_test.ravel(), verbose=2)\n\nprint('\\nTest accuracy:', test_acc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ypre=model.predict(X_test)\nypre= np.ravel(ypre)\nypre.shape\nypr=(ypre>0.5)*1\nfrom sklearn.metrics import classification_report, confusion_matrix\nprint(confusion_matrix(y_test.ravel(),ypr))\nprint(classification_report(y_test.ravel(),ypr))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def callf1(xx,yy,xt,yt):\n    model = tf.keras.Sequential([\n        tf.keras.layers.Dense(256, activation='relu'),\n        tf.keras.layers.Dense(128, activation='relu'),\n        tf.keras.layers.Dense(1)\n    ])\n    \n    model.compile(optimizer='adam',\n                  loss='mean_absolute_error',\n                  metrics=['accuracy'])\n    model.fit(xx, yy , epochs=3)\n    ls=[]\n    test_loss, test_acc = model.evaluate(xt,  yt, verbose=2)\n    print('\\nTest accuracy:', test_acc)\n    tr_loss, tr_acc = model.evaluate(xx,  yy, verbose=2)\n    ls.append(test_acc)\n    ls.append(tr_acc)\n    ypr=model.predict(xt)\n    ypr=(ypr>0.5)*1\n    ypre= np.ravel(ypr)\n    ls.append(f1_score(yt, ypre))\n    return ls","metadata":{"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"X_train.shape","metadata":{"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"(301312, 42)"},"metadata":{}}]},{"cell_type":"code","source":"r=callf1(X_train, y_train.ravel(),X_test,y_test.ravel())\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r=callf1(X_train_res,y_train_res.ravel(),X_test,y_test.ravel())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sta=[]\nste=[]\nsf =[]\nfor i in range(30):\n    r=callf1(X_train, y_train.ravel(),X_test,y_test.ravel())\n    ste.append(r[0])\n    sta.append(r[1])\n    sf.append(r[2])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sum(sf)/30","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sum(sta)/30","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sum(ste)/30","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max(sf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max(sta)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max(ste)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stdev(sf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stdev(sta)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stdev(ste)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sta2=[]\nste2=[]\nsf2 =[]\nfor i in range(30):\n    r=callf1(X_train_res,y_train_res.ravel(),X_test,y_test.ravel())\n    ste2.append(r[0])\n    sta2.append(r[1])\n    sf2.append(r[2])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sum(sf2)/30","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max(sf2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stdev(sf2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sum(ste2)/30","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max(ste2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stdev(ste2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sum(sta2)/30","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max(sta2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stdev(sta2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save=1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"progress=1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save2=1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_oversampled=X_train_res[301312:]","metadata":{"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom tqdm.auto import tqdm\nfrom torchvision import transforms\nfrom torchvision.utils import make_grid\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"def get_generator_block(input_dim, output_dim):\n    return nn.Sequential(\n        nn.Linear(input_dim, output_dim),\n        nn.BatchNorm1d(output_dim),\n        nn.ReLU(inplace=True),\n    )","metadata":{"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"class Generator(nn.Module):\n\n    def __init__(self, z_dim=42, im_dim=42, hidden_dim=128):\n        super(Generator, self).__init__()\n        self.gen = nn.Sequential(\n            get_generator_block(z_dim, hidden_dim),\n            get_generator_block(hidden_dim, hidden_dim * 2),\n            get_generator_block(hidden_dim * 2, hidden_dim * 4),\n            get_generator_block(hidden_dim * 4, hidden_dim * 8),\n            nn.Linear(hidden_dim * 8, im_dim),\n            nn.Sigmoid()\n        )\n    def forward(self, noise):\n        return self.gen(noise)\n    \n    # Needed for grading\n    def get_gen(self):\n\n        return self.gen","metadata":{"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"def get_discriminator_block(input_dim, output_dim):\n    return nn.Sequential(\n        nn.Linear(input_dim, output_dim),\n        nn.LeakyReLU(0.2, inplace=True)        \n    )","metadata":{"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    def __init__(self, im_dim=42, hidden_dim=128):\n        super(Discriminator, self).__init__()\n        self.disc = nn.Sequential(\n            get_discriminator_block(im_dim, hidden_dim * 4),\n            get_discriminator_block(hidden_dim * 4, hidden_dim * 2),\n            get_discriminator_block(hidden_dim * 2, hidden_dim),\n            nn.Linear(hidden_dim, 1)\n        )\n\n    def forward(self, image):\n\n        return self.disc(image)\n    \n    def get_disc(self):\n\n        return self.dis","metadata":{"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"X_oversampled = torch.from_numpy(X_oversampled)","metadata":{"trusted":true},"execution_count":49,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-49-5a539c9e36c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_oversampled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_oversampled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mTypeError\u001b[0m: expected np.ndarray (got Tensor)"],"ename":"TypeError","evalue":"expected np.ndarray (got Tensor)","output_type":"error"}]},{"cell_type":"code","source":"X_oversampled.shape","metadata":{"trusted":true},"execution_count":50,"outputs":[{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"torch.Size([278188, 42])"},"metadata":{}}]},{"cell_type":"code","source":"criterion = nn.BCEWithLogitsLoss()\nn_epochs = 5\nz_dim = 42\nbatch_size = 128\nlr = 0.00001\ndisplay_step = 1\ndevice = 'cuda'","metadata":{"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"gen = Generator(z_dim).to(device)\ngen_opt = torch.optim.Adam(gen.parameters(), lr=lr)\ndisc = Discriminator().to(device) \ndisc_opt = torch.optim.Adam(disc.parameters(), lr=lr)","metadata":{"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"def get_disc_loss(gen, disc, criterion, real, device):\n\n    fake = gen(X_oversampled.float().to(device))\n    disc_fake_pred = disc(fake.detach())\n    disc_fake_loss = criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))\n    disc_real_pred = disc(real)\n    disc_real_loss = criterion(disc_real_pred, torch.ones_like(disc_real_pred))\n    disc_loss = (disc_fake_loss + disc_real_loss) / 2\n    return disc_loss","metadata":{"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"def get_gen_loss(gen, disc, criterion, num_images, z_dim, device):\n    fake_images = gen(X_oversampled.float().to(device))\n    \n    disc_fake_pred = disc(fake_images)\n    gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred))\n    return gen_loss","metadata":{"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"y_tr=y_train.ravel()","metadata":{"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"li=[]\nfor i in range(len(y_tr)):\n    if int(y_tr[i])==1:\n        li.append(X_train[i])","metadata":{"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"X_real=np.array(li)","metadata":{"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"X_real.shape","metadata":{"trusted":true},"execution_count":58,"outputs":[{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"(11562, 42)"},"metadata":{}}]},{"cell_type":"code","source":"li2=[1]*11562","metadata":{"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"y_real=np.array(li2)\ny_real.shape","metadata":{"trusted":true},"execution_count":60,"outputs":[{"execution_count":60,"output_type":"execute_result","data":{"text/plain":"(11562,)"},"metadata":{}}]},{"cell_type":"code","source":"from torch.utils.data import TensorDataset, DataLoader\ntensor_x = torch.Tensor(X_real) \ntensor_y = torch.Tensor(y_real)\nmy_dataset = TensorDataset(tensor_x,tensor_y) # create your datset","metadata":{"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"dataloader = DataLoader(\n    my_dataset,\n    batch_size=batch_size,\n    shuffle=True)","metadata":{"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"\ncur_step = 0\nmean_generator_loss = 0\nmean_discriminator_loss = 0\ntest_generator = True \ngen_loss = False\nerror = False\nfor epoch in range(n_epochs):\n  \n    # Dataloader returns the batches\n    for real, _ in tqdm(dataloader):\n        cur_batch_size = len(real)\n\n        # Flatten the batch of real images from the dataset\n        real = real.view(cur_batch_size, -1).to(device)\n\n        ### Update discriminator ###\n        # Zero out the gradients before backpropagation\n        disc_opt.zero_grad()\n\n        # Calculate discriminator loss\n        disc_loss = get_disc_loss(gen, disc, criterion, real, device)\n\n        # Update gradients#\n        disc_loss.backward(retain_graph=True)\n\n        # Update optimizer\n        disc_opt.step()\n\n        # For testing purposes, to keep track of the generator weights\n        if test_generator:\n            old_generator_weights = gen.gen[0][0].weight.detach().clone()\n\n        ### Update generator ###\n        #     Hint: This code will look a lot like the discriminator updates!\n        #     These are the steps you will need to complete:\n        #       1) Zero out the gradients.\n        #       2) Calculate the generator loss, assigning it to gen_loss.\n        #       3) Backprop through the generator: update the gradients and optimizer.\n        #### START CODE HERE ####\n        gen_opt.zero_grad()\n        gen_loss = get_gen_loss(gen, disc, criterion, cur_batch_size, z_dim, device)\n        gen_loss.backward()\n        gen_opt.step()        \n        #### END CODE HERE ####\n\n        # For testing purposes, to check that your code changes the generator weights\n        if test_generator:\n            try:\n                assert lr > 0.0000002 or (gen.gen[0][0].weight.grad.abs().max() < 0.0005 and epoch == 0)\n                assert torch.any(gen.gen[0][0].weight.detach().clone() != old_generator_weights)\n            except:\n                error = True\n                print(\"Runtime tests have failed\")\n\n        # Keep track of the average discriminator loss\n        mean_discriminator_loss += disc_loss.item() / display_step\n\n        # Keep track of the average generator loss\n        mean_generator_loss += gen_loss.item() / display_step\n\n        ### Visualization code ###\n        if cur_step % display_step == 0 and cur_step > 0:\n            print(f\"Epoch {epoch}, step {cur_step}: Generator loss: {mean_generator_loss}, discriminator loss: {mean_discriminator_loss}\")\n            #fake_noise = get_noise(cur_batch_size, z_dim, device=device)\n            #fake = gen(fake_noise)\n            #show_tensor_images(fake)\n            #show_tensor_images(real)\n            mean_generator_loss = 0\n            mean_discriminator_loss = 0\n        cur_step += 1\n","metadata":{"trusted":true},"execution_count":63,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/91 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89fc0456e2bf40f58dc4f4808b5ba31a"}},"metadata":{}},{"name":"stdout","text":"Epoch 0, step 1: Generator loss: 1.3323635458946228, discriminator loss: 1.3838247060775757\nEpoch 0, step 2: Generator loss: 0.6671561002731323, discriminator loss: 0.6915805339813232\nEpoch 0, step 3: Generator loss: 0.6678057312965393, discriminator loss: 0.6891282796859741\nEpoch 0, step 4: Generator loss: 0.6684545874595642, discriminator loss: 0.6902801990509033\nEpoch 0, step 5: Generator loss: 0.6691085696220398, discriminator loss: 0.6879838109016418\nEpoch 0, step 6: Generator loss: 0.6697635054588318, discriminator loss: 0.6880554556846619\nEpoch 0, step 7: Generator loss: 0.6704192757606506, discriminator loss: 0.6867396831512451\nEpoch 0, step 8: Generator loss: 0.6710783243179321, discriminator loss: 0.6862133741378784\nEpoch 0, step 9: Generator loss: 0.671737551689148, discriminator loss: 0.6860945224761963\nEpoch 0, step 10: Generator loss: 0.6724002957344055, discriminator loss: 0.6849404573440552\nEpoch 0, step 11: Generator loss: 0.6730656623840332, discriminator loss: 0.6844631433486938\nEpoch 0, step 12: Generator loss: 0.6737310886383057, discriminator loss: 0.6849737167358398\nEpoch 0, step 13: Generator loss: 0.6743979454040527, discriminator loss: 0.683202862739563\nEpoch 0, step 14: Generator loss: 0.6750651597976685, discriminator loss: 0.6832596659660339\nEpoch 0, step 15: Generator loss: 0.6757314205169678, discriminator loss: 0.68224036693573\nEpoch 0, step 16: Generator loss: 0.6763982772827148, discriminator loss: 0.6828516125679016\nEpoch 0, step 17: Generator loss: 0.6770650744438171, discriminator loss: 0.6806540489196777\nEpoch 0, step 18: Generator loss: 0.6777326464653015, discriminator loss: 0.6797356605529785\nEpoch 0, step 19: Generator loss: 0.67840176820755, discriminator loss: 0.6798157691955566\nEpoch 0, step 20: Generator loss: 0.6790717840194702, discriminator loss: 0.6792832612991333\nEpoch 0, step 21: Generator loss: 0.6797433495521545, discriminator loss: 0.678013265132904\nEpoch 0, step 22: Generator loss: 0.6804150938987732, discriminator loss: 0.6776995062828064\nEpoch 0, step 23: Generator loss: 0.681087076663971, discriminator loss: 0.6759434938430786\nEpoch 0, step 24: Generator loss: 0.6817587018013, discriminator loss: 0.6758027672767639\nEpoch 0, step 25: Generator loss: 0.6824280023574829, discriminator loss: 0.6758822202682495\nEpoch 0, step 26: Generator loss: 0.6830941438674927, discriminator loss: 0.6749631762504578\nEpoch 0, step 27: Generator loss: 0.6837608218193054, discriminator loss: 0.673906683921814\nEpoch 0, step 28: Generator loss: 0.6844249963760376, discriminator loss: 0.6746636629104614\nEpoch 0, step 29: Generator loss: 0.6850876212120056, discriminator loss: 0.6736001372337341\nEpoch 0, step 30: Generator loss: 0.6857472658157349, discriminator loss: 0.6728250980377197\nEpoch 0, step 31: Generator loss: 0.6864034533500671, discriminator loss: 0.6718355417251587\nEpoch 0, step 32: Generator loss: 0.6870573163032532, discriminator loss: 0.6706905364990234\nEpoch 0, step 33: Generator loss: 0.6877090334892273, discriminator loss: 0.6703561544418335\nEpoch 0, step 34: Generator loss: 0.6883602738380432, discriminator loss: 0.6700109839439392\nEpoch 0, step 35: Generator loss: 0.6890082359313965, discriminator loss: 0.6698778867721558\nEpoch 0, step 36: Generator loss: 0.6896539926528931, discriminator loss: 0.6677476167678833\nEpoch 0, step 37: Generator loss: 0.6902984380722046, discriminator loss: 0.6680639982223511\nEpoch 0, step 38: Generator loss: 0.6909391283988953, discriminator loss: 0.6664137840270996\nEpoch 0, step 39: Generator loss: 0.6915765404701233, discriminator loss: 0.6672426462173462\nEpoch 0, step 40: Generator loss: 0.6922131180763245, discriminator loss: 0.666621208190918\nEpoch 0, step 41: Generator loss: 0.6928475499153137, discriminator loss: 0.6657291650772095\nEpoch 0, step 42: Generator loss: 0.6934818029403687, discriminator loss: 0.6647329926490784\nEpoch 0, step 43: Generator loss: 0.6941163539886475, discriminator loss: 0.6639488339424133\nEpoch 0, step 44: Generator loss: 0.6947510838508606, discriminator loss: 0.6629321575164795\nEpoch 0, step 45: Generator loss: 0.6953843832015991, discriminator loss: 0.6633806228637695\nEpoch 0, step 46: Generator loss: 0.6960172653198242, discriminator loss: 0.6613813638687134\nEpoch 0, step 47: Generator loss: 0.6966482996940613, discriminator loss: 0.6615958213806152\nEpoch 0, step 48: Generator loss: 0.6972784399986267, discriminator loss: 0.6614857912063599\nEpoch 0, step 49: Generator loss: 0.6979086399078369, discriminator loss: 0.6596121191978455\nEpoch 0, step 50: Generator loss: 0.6985395550727844, discriminator loss: 0.6589739918708801\nEpoch 0, step 51: Generator loss: 0.6991710066795349, discriminator loss: 0.6591576337814331\nEpoch 0, step 52: Generator loss: 0.6998045444488525, discriminator loss: 0.6581952571868896\nEpoch 0, step 53: Generator loss: 0.7004383206367493, discriminator loss: 0.658186674118042\nEpoch 0, step 54: Generator loss: 0.701073408126831, discriminator loss: 0.6564397811889648\nEpoch 0, step 55: Generator loss: 0.7017077803611755, discriminator loss: 0.654977560043335\nEpoch 0, step 56: Generator loss: 0.7023425102233887, discriminator loss: 0.6561412215232849\nEpoch 0, step 57: Generator loss: 0.7029774188995361, discriminator loss: 0.6560120582580566\nEpoch 0, step 58: Generator loss: 0.7036094069480896, discriminator loss: 0.6548551917076111\nEpoch 0, step 59: Generator loss: 0.7042392492294312, discriminator loss: 0.6543586254119873\nEpoch 0, step 60: Generator loss: 0.7048640251159668, discriminator loss: 0.654363751411438\nEpoch 0, step 61: Generator loss: 0.7054863572120667, discriminator loss: 0.6541600227355957\nEpoch 0, step 62: Generator loss: 0.7061063647270203, discriminator loss: 0.6517784595489502\nEpoch 0, step 63: Generator loss: 0.7067256569862366, discriminator loss: 0.6528641581535339\nEpoch 0, step 64: Generator loss: 0.7073445320129395, discriminator loss: 0.6507331132888794\nEpoch 0, step 65: Generator loss: 0.7079616189002991, discriminator loss: 0.6501855850219727\nEpoch 0, step 66: Generator loss: 0.7085756659507751, discriminator loss: 0.6502809524536133\nEpoch 0, step 67: Generator loss: 0.709186315536499, discriminator loss: 0.6498557329177856\nEpoch 0, step 68: Generator loss: 0.7097927331924438, discriminator loss: 0.6481000185012817\nEpoch 0, step 69: Generator loss: 0.7103970050811768, discriminator loss: 0.6481393575668335\nEpoch 0, step 70: Generator loss: 0.710997462272644, discriminator loss: 0.6473239660263062\nEpoch 0, step 71: Generator loss: 0.7115961909294128, discriminator loss: 0.6461034417152405\nEpoch 0, step 72: Generator loss: 0.7121917009353638, discriminator loss: 0.6438568830490112\nEpoch 0, step 73: Generator loss: 0.7127859592437744, discriminator loss: 0.6457248330116272\nEpoch 0, step 74: Generator loss: 0.7133761048316956, discriminator loss: 0.6449920535087585\nEpoch 0, step 75: Generator loss: 0.7139635682106018, discriminator loss: 0.6431965827941895\nEpoch 0, step 76: Generator loss: 0.7145455479621887, discriminator loss: 0.6435490250587463\nEpoch 0, step 77: Generator loss: 0.7151225209236145, discriminator loss: 0.6430421471595764\nEpoch 0, step 78: Generator loss: 0.7156966328620911, discriminator loss: 0.6420615315437317\nEpoch 0, step 79: Generator loss: 0.7162684202194214, discriminator loss: 0.6426692605018616\nEpoch 0, step 80: Generator loss: 0.7168371677398682, discriminator loss: 0.6404598951339722\nEpoch 0, step 81: Generator loss: 0.717402458190918, discriminator loss: 0.6407800912857056\nEpoch 0, step 82: Generator loss: 0.7179667353630066, discriminator loss: 0.6400116682052612\nEpoch 0, step 83: Generator loss: 0.7185269594192505, discriminator loss: 0.6383266448974609\nEpoch 0, step 84: Generator loss: 0.7190844416618347, discriminator loss: 0.6386407613754272\nEpoch 0, step 85: Generator loss: 0.7196415662765503, discriminator loss: 0.6377148628234863\nEpoch 0, step 86: Generator loss: 0.720198929309845, discriminator loss: 0.6348565816879272\nEpoch 0, step 87: Generator loss: 0.7207550406455994, discriminator loss: 0.6365609169006348\nEpoch 0, step 88: Generator loss: 0.7213084697723389, discriminator loss: 0.6347423195838928\nEpoch 0, step 89: Generator loss: 0.7218600511550903, discriminator loss: 0.6349811553955078\nEpoch 0, step 90: Generator loss: 0.7224076986312866, discriminator loss: 0.6363694667816162\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/91 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a40e6d4e8f0c41eda94669fa431b7b4a"}},"metadata":{}},{"name":"stdout","text":"Epoch 1, step 91: Generator loss: 0.7229529619216919, discriminator loss: 0.6336230039596558\nEpoch 1, step 92: Generator loss: 0.7234956622123718, discriminator loss: 0.632878303527832\nEpoch 1, step 93: Generator loss: 0.724037230014801, discriminator loss: 0.632204532623291\nEpoch 1, step 94: Generator loss: 0.724579930305481, discriminator loss: 0.6313784122467041\nEpoch 1, step 95: Generator loss: 0.7251205444335938, discriminator loss: 0.6295487284660339\nEpoch 1, step 96: Generator loss: 0.7256585359573364, discriminator loss: 0.6303982734680176\nEpoch 1, step 97: Generator loss: 0.7261976599693298, discriminator loss: 0.6304221749305725\nEpoch 1, step 98: Generator loss: 0.7267338037490845, discriminator loss: 0.6307727098464966\nEpoch 1, step 99: Generator loss: 0.727266788482666, discriminator loss: 0.6284278035163879\nEpoch 1, step 100: Generator loss: 0.7277980446815491, discriminator loss: 0.6278976202011108\nEpoch 1, step 101: Generator loss: 0.7283294200897217, discriminator loss: 0.6267342567443848\nEpoch 1, step 102: Generator loss: 0.728859543800354, discriminator loss: 0.6251720190048218\nEpoch 1, step 103: Generator loss: 0.7293902635574341, discriminator loss: 0.6264998912811279\nEpoch 1, step 104: Generator loss: 0.7299224138259888, discriminator loss: 0.622779905796051\nEpoch 1, step 105: Generator loss: 0.7304548025131226, discriminator loss: 0.6232240200042725\nEpoch 1, step 106: Generator loss: 0.7309839725494385, discriminator loss: 0.6231945753097534\nEpoch 1, step 107: Generator loss: 0.7315148711204529, discriminator loss: 0.6230663061141968\nEpoch 1, step 108: Generator loss: 0.7320465445518494, discriminator loss: 0.6221741437911987\nEpoch 1, step 109: Generator loss: 0.7325767278671265, discriminator loss: 0.6209918260574341\nEpoch 1, step 110: Generator loss: 0.7331070899963379, discriminator loss: 0.6206332445144653\nEpoch 1, step 111: Generator loss: 0.7336382269859314, discriminator loss: 0.6204641461372375\nEpoch 1, step 112: Generator loss: 0.7341691851615906, discriminator loss: 0.6192617416381836\nEpoch 1, step 113: Generator loss: 0.7346994280815125, discriminator loss: 0.6180453896522522\nEpoch 1, step 114: Generator loss: 0.7352265119552612, discriminator loss: 0.6173728704452515\nEpoch 1, step 115: Generator loss: 0.735755205154419, discriminator loss: 0.617603063583374\nEpoch 1, step 116: Generator loss: 0.736284077167511, discriminator loss: 0.6160943508148193\nEpoch 1, step 117: Generator loss: 0.7368084788322449, discriminator loss: 0.615570604801178\nEpoch 1, step 118: Generator loss: 0.7373306155204773, discriminator loss: 0.6132473945617676\nEpoch 1, step 119: Generator loss: 0.7378517985343933, discriminator loss: 0.6129745244979858\nEpoch 1, step 120: Generator loss: 0.7383713126182556, discriminator loss: 0.6134865283966064\nEpoch 1, step 121: Generator loss: 0.7388899326324463, discriminator loss: 0.6137986183166504\nEpoch 1, step 122: Generator loss: 0.7394039630889893, discriminator loss: 0.6128556728363037\nEpoch 1, step 123: Generator loss: 0.7399184107780457, discriminator loss: 0.6104140877723694\nEpoch 1, step 124: Generator loss: 0.740429699420929, discriminator loss: 0.6097687482833862\nEpoch 1, step 125: Generator loss: 0.7409408688545227, discriminator loss: 0.6081331968307495\nEpoch 1, step 126: Generator loss: 0.7414512634277344, discriminator loss: 0.609618067741394\nEpoch 1, step 127: Generator loss: 0.741960346698761, discriminator loss: 0.6087297797203064\nEpoch 1, step 128: Generator loss: 0.7424693703651428, discriminator loss: 0.6091670393943787\nEpoch 1, step 129: Generator loss: 0.7429781556129456, discriminator loss: 0.605915904045105\nEpoch 1, step 130: Generator loss: 0.7434876561164856, discriminator loss: 0.6068363189697266\nEpoch 1, step 131: Generator loss: 0.743995726108551, discriminator loss: 0.6053586006164551\nEpoch 1, step 132: Generator loss: 0.744502604007721, discriminator loss: 0.6045129895210266\nEpoch 1, step 133: Generator loss: 0.7450062036514282, discriminator loss: 0.6051645874977112\nEpoch 1, step 134: Generator loss: 0.745507001876831, discriminator loss: 0.6049185991287231\nEpoch 1, step 135: Generator loss: 0.746005654335022, discriminator loss: 0.6028869152069092\nEpoch 1, step 136: Generator loss: 0.7465019226074219, discriminator loss: 0.601486325263977\nEpoch 1, step 137: Generator loss: 0.7469942569732666, discriminator loss: 0.602280855178833\nEpoch 1, step 138: Generator loss: 0.7474873065948486, discriminator loss: 0.6004921793937683\nEpoch 1, step 139: Generator loss: 0.7479823231697083, discriminator loss: 0.5980868339538574\nEpoch 1, step 140: Generator loss: 0.7484760880470276, discriminator loss: 0.6007607579231262\nEpoch 1, step 141: Generator loss: 0.7489697933197021, discriminator loss: 0.5983015298843384\nEpoch 1, step 142: Generator loss: 0.7494621276855469, discriminator loss: 0.5982358455657959\nEpoch 1, step 143: Generator loss: 0.749954104423523, discriminator loss: 0.5963734984397888\nEpoch 1, step 144: Generator loss: 0.7504443526268005, discriminator loss: 0.5961025953292847\nEpoch 1, step 145: Generator loss: 0.7509348392486572, discriminator loss: 0.5953013896942139\nEpoch 1, step 146: Generator loss: 0.7514254450798035, discriminator loss: 0.5948473215103149\nEpoch 1, step 147: Generator loss: 0.7519156336784363, discriminator loss: 0.5938327312469482\nEpoch 1, step 148: Generator loss: 0.7524067759513855, discriminator loss: 0.5918760299682617\nEpoch 1, step 149: Generator loss: 0.7528931498527527, discriminator loss: 0.5915641784667969\nEpoch 1, step 150: Generator loss: 0.7533766627311707, discriminator loss: 0.5918210744857788\nEpoch 1, step 151: Generator loss: 0.7538565993309021, discriminator loss: 0.591242790222168\nEpoch 1, step 152: Generator loss: 0.7543324828147888, discriminator loss: 0.5891626477241516\nEpoch 1, step 153: Generator loss: 0.7548028826713562, discriminator loss: 0.5896399021148682\nEpoch 1, step 154: Generator loss: 0.7552729845046997, discriminator loss: 0.5879465341567993\nEpoch 1, step 155: Generator loss: 0.7557438611984253, discriminator loss: 0.5867595672607422\nEpoch 1, step 156: Generator loss: 0.7562139630317688, discriminator loss: 0.5878322720527649\nEpoch 1, step 157: Generator loss: 0.7566814422607422, discriminator loss: 0.5873688459396362\nEpoch 1, step 158: Generator loss: 0.7571496367454529, discriminator loss: 0.5845664739608765\nEpoch 1, step 159: Generator loss: 0.757615864276886, discriminator loss: 0.5845561027526855\nEpoch 1, step 160: Generator loss: 0.7580777406692505, discriminator loss: 0.5845308303833008\nEpoch 1, step 161: Generator loss: 0.7585424780845642, discriminator loss: 0.5853043794631958\nEpoch 1, step 162: Generator loss: 0.7590062618255615, discriminator loss: 0.5831828117370605\nEpoch 1, step 163: Generator loss: 0.7594685554504395, discriminator loss: 0.583534300327301\nEpoch 1, step 164: Generator loss: 0.7599310278892517, discriminator loss: 0.5804399251937866\nEpoch 1, step 165: Generator loss: 0.7603914141654968, discriminator loss: 0.5813954472541809\nEpoch 1, step 166: Generator loss: 0.7608489990234375, discriminator loss: 0.579780101776123\nEpoch 1, step 167: Generator loss: 0.7613036036491394, discriminator loss: 0.5796382427215576\nEpoch 1, step 168: Generator loss: 0.7617553472518921, discriminator loss: 0.5788787603378296\nEpoch 1, step 169: Generator loss: 0.7622054219245911, discriminator loss: 0.5765789747238159\nEpoch 1, step 170: Generator loss: 0.7626516222953796, discriminator loss: 0.5768986940383911\nEpoch 1, step 171: Generator loss: 0.7630972266197205, discriminator loss: 0.5757640600204468\nEpoch 1, step 172: Generator loss: 0.763539731502533, discriminator loss: 0.5761207938194275\nEpoch 1, step 173: Generator loss: 0.763976514339447, discriminator loss: 0.5746496915817261\nEpoch 1, step 174: Generator loss: 0.7644107341766357, discriminator loss: 0.5731448531150818\nEpoch 1, step 175: Generator loss: 0.7648456692695618, discriminator loss: 0.571088433265686\nEpoch 1, step 176: Generator loss: 0.7652786374092102, discriminator loss: 0.5718507766723633\nEpoch 1, step 177: Generator loss: 0.7657122015953064, discriminator loss: 0.5684202909469604\nEpoch 1, step 178: Generator loss: 0.7661455273628235, discriminator loss: 0.5706188678741455\nEpoch 1, step 179: Generator loss: 0.7665767073631287, discriminator loss: 0.5689196586608887\nEpoch 1, step 180: Generator loss: 0.7670024037361145, discriminator loss: 0.568462610244751\nEpoch 1, step 181: Generator loss: 0.7674254775047302, discriminator loss: 0.5687446594238281\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/91 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61fb885ffa66433396aa929357b89143"}},"metadata":{}},{"name":"stdout","text":"Epoch 2, step 182: Generator loss: 0.7678455710411072, discriminator loss: 0.5669853687286377\nEpoch 2, step 183: Generator loss: 0.7682636380195618, discriminator loss: 0.564589262008667\nEpoch 2, step 184: Generator loss: 0.768679678440094, discriminator loss: 0.564838171005249\nEpoch 2, step 185: Generator loss: 0.7690938711166382, discriminator loss: 0.5632424354553223\nEpoch 2, step 186: Generator loss: 0.7695046663284302, discriminator loss: 0.56485515832901\nEpoch 2, step 187: Generator loss: 0.7699155211448669, discriminator loss: 0.562430739402771\nEpoch 2, step 188: Generator loss: 0.7703229784965515, discriminator loss: 0.5628600120544434\nEpoch 2, step 189: Generator loss: 0.7707279920578003, discriminator loss: 0.5601676106452942\nEpoch 2, step 190: Generator loss: 0.771128237247467, discriminator loss: 0.5585938096046448\nEpoch 2, step 191: Generator loss: 0.7715260982513428, discriminator loss: 0.5603410005569458\nEpoch 2, step 192: Generator loss: 0.7719219923019409, discriminator loss: 0.5579270124435425\nEpoch 2, step 193: Generator loss: 0.7723153829574585, discriminator loss: 0.5559959411621094\nEpoch 2, step 194: Generator loss: 0.7727054357528687, discriminator loss: 0.5575118660926819\nEpoch 2, step 195: Generator loss: 0.7730913162231445, discriminator loss: 0.5557659268379211\nEpoch 2, step 196: Generator loss: 0.7734731435775757, discriminator loss: 0.5543277263641357\nEpoch 2, step 197: Generator loss: 0.7738500237464905, discriminator loss: 0.5566049814224243\nEpoch 2, step 198: Generator loss: 0.7742204666137695, discriminator loss: 0.5551203489303589\nEpoch 2, step 199: Generator loss: 0.7745908498764038, discriminator loss: 0.5525206327438354\nEpoch 2, step 200: Generator loss: 0.7749598622322083, discriminator loss: 0.5529531240463257\nEpoch 2, step 201: Generator loss: 0.7753275632858276, discriminator loss: 0.5496190786361694\nEpoch 2, step 202: Generator loss: 0.7756951451301575, discriminator loss: 0.5492321252822876\nEpoch 2, step 203: Generator loss: 0.7760643362998962, discriminator loss: 0.5483273863792419\nEpoch 2, step 204: Generator loss: 0.7764310836791992, discriminator loss: 0.5456598401069641\nEpoch 2, step 205: Generator loss: 0.7767922282218933, discriminator loss: 0.5503644943237305\nEpoch 2, step 206: Generator loss: 0.7771493792533875, discriminator loss: 0.5475126504898071\nEpoch 2, step 207: Generator loss: 0.777502179145813, discriminator loss: 0.5464761853218079\nEpoch 2, step 208: Generator loss: 0.7778527736663818, discriminator loss: 0.546012818813324\nEpoch 2, step 209: Generator loss: 0.7782014012336731, discriminator loss: 0.5448154211044312\nEpoch 2, step 210: Generator loss: 0.7785473465919495, discriminator loss: 0.5446240901947021\nEpoch 2, step 211: Generator loss: 0.7788919806480408, discriminator loss: 0.5434194207191467\nEpoch 2, step 212: Generator loss: 0.7792346477508545, discriminator loss: 0.5416591763496399\nEpoch 2, step 213: Generator loss: 0.779574453830719, discriminator loss: 0.5406105518341064\nEpoch 2, step 214: Generator loss: 0.7799105644226074, discriminator loss: 0.540087103843689\nEpoch 2, step 215: Generator loss: 0.7802413702011108, discriminator loss: 0.5402471423149109\nEpoch 2, step 216: Generator loss: 0.7805673480033875, discriminator loss: 0.5406501889228821\nEpoch 2, step 217: Generator loss: 0.7808899879455566, discriminator loss: 0.5377497673034668\nEpoch 2, step 218: Generator loss: 0.7812097072601318, discriminator loss: 0.535968542098999\nEpoch 2, step 219: Generator loss: 0.7815215587615967, discriminator loss: 0.5372622013092041\nEpoch 2, step 220: Generator loss: 0.7818332314491272, discriminator loss: 0.5372017025947571\nEpoch 2, step 221: Generator loss: 0.782139778137207, discriminator loss: 0.5356191396713257\nEpoch 2, step 222: Generator loss: 0.7824444770812988, discriminator loss: 0.5353348851203918\nEpoch 2, step 223: Generator loss: 0.782747209072113, discriminator loss: 0.5355063080787659\nEpoch 2, step 224: Generator loss: 0.7830505967140198, discriminator loss: 0.5326415300369263\nEpoch 2, step 225: Generator loss: 0.7833489775657654, discriminator loss: 0.5309647917747498\nEpoch 2, step 226: Generator loss: 0.7836456894874573, discriminator loss: 0.5317453145980835\nEpoch 2, step 227: Generator loss: 0.783937931060791, discriminator loss: 0.5307585000991821\nEpoch 2, step 228: Generator loss: 0.7842239141464233, discriminator loss: 0.5284548401832581\nEpoch 2, step 229: Generator loss: 0.7845087051391602, discriminator loss: 0.5271186232566833\nEpoch 2, step 230: Generator loss: 0.784788966178894, discriminator loss: 0.5277845859527588\nEpoch 2, step 231: Generator loss: 0.7850697040557861, discriminator loss: 0.5270888805389404\nEpoch 2, step 232: Generator loss: 0.7853492498397827, discriminator loss: 0.526585042476654\nEpoch 2, step 233: Generator loss: 0.7856268286705017, discriminator loss: 0.523739755153656\nEpoch 2, step 234: Generator loss: 0.7859011292457581, discriminator loss: 0.5242874622344971\nEpoch 2, step 235: Generator loss: 0.7861688733100891, discriminator loss: 0.5263397097587585\nEpoch 2, step 236: Generator loss: 0.7864348888397217, discriminator loss: 0.5214433670043945\nEpoch 2, step 237: Generator loss: 0.7866963744163513, discriminator loss: 0.5227178335189819\nEpoch 2, step 238: Generator loss: 0.7869554758071899, discriminator loss: 0.521615207195282\nEpoch 2, step 239: Generator loss: 0.7872093915939331, discriminator loss: 0.5201264023780823\nEpoch 2, step 240: Generator loss: 0.7874558568000793, discriminator loss: 0.5190842151641846\nEpoch 2, step 241: Generator loss: 0.7877033948898315, discriminator loss: 0.5179305076599121\nEpoch 2, step 242: Generator loss: 0.7879505157470703, discriminator loss: 0.5170121192932129\nEpoch 2, step 243: Generator loss: 0.7881977558135986, discriminator loss: 0.5177994966506958\nEpoch 2, step 244: Generator loss: 0.7884438037872314, discriminator loss: 0.5143237113952637\nEpoch 2, step 245: Generator loss: 0.7886857390403748, discriminator loss: 0.51594078540802\nEpoch 2, step 246: Generator loss: 0.7889282703399658, discriminator loss: 0.5137239694595337\nEpoch 2, step 247: Generator loss: 0.7891682386398315, discriminator loss: 0.5143560767173767\nEpoch 2, step 248: Generator loss: 0.7894023060798645, discriminator loss: 0.51544189453125\nEpoch 2, step 249: Generator loss: 0.789635419845581, discriminator loss: 0.512994110584259\nEpoch 2, step 250: Generator loss: 0.7898675203323364, discriminator loss: 0.5128633975982666\nEpoch 2, step 251: Generator loss: 0.7900947332382202, discriminator loss: 0.5115006566047668\nEpoch 2, step 252: Generator loss: 0.7903193831443787, discriminator loss: 0.509283185005188\nEpoch 2, step 253: Generator loss: 0.7905444502830505, discriminator loss: 0.5094316005706787\nEpoch 2, step 254: Generator loss: 0.7907679677009583, discriminator loss: 0.505618691444397\nEpoch 2, step 255: Generator loss: 0.7909910678863525, discriminator loss: 0.5051625370979309\nEpoch 2, step 256: Generator loss: 0.7912108302116394, discriminator loss: 0.5057687163352966\nEpoch 2, step 257: Generator loss: 0.7914294004440308, discriminator loss: 0.5059283971786499\nEpoch 2, step 258: Generator loss: 0.7916477918624878, discriminator loss: 0.5029895901679993\nEpoch 2, step 259: Generator loss: 0.791865348815918, discriminator loss: 0.5023794770240784\nEpoch 2, step 260: Generator loss: 0.7920796871185303, discriminator loss: 0.5037839412689209\nEpoch 2, step 261: Generator loss: 0.7922940254211426, discriminator loss: 0.502047598361969\nEpoch 2, step 262: Generator loss: 0.7925065755844116, discriminator loss: 0.5035082697868347\nEpoch 2, step 263: Generator loss: 0.7927200794219971, discriminator loss: 0.500677227973938\nEpoch 2, step 264: Generator loss: 0.7929294109344482, discriminator loss: 0.49939948320388794\nEpoch 2, step 265: Generator loss: 0.7931345701217651, discriminator loss: 0.4986054301261902\nEpoch 2, step 266: Generator loss: 0.7933403253555298, discriminator loss: 0.49837779998779297\nEpoch 2, step 267: Generator loss: 0.7935410737991333, discriminator loss: 0.49737831950187683\nEpoch 2, step 268: Generator loss: 0.7937382459640503, discriminator loss: 0.49962490797042847\nEpoch 2, step 269: Generator loss: 0.7939305901527405, discriminator loss: 0.49691635370254517\nEpoch 2, step 270: Generator loss: 0.7941184043884277, discriminator loss: 0.49752670526504517\nEpoch 2, step 271: Generator loss: 0.7943025827407837, discriminator loss: 0.49470949172973633\nEpoch 2, step 272: Generator loss: 0.7944861054420471, discriminator loss: 0.49580878019332886\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/91 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb5bc4ee01354518bda29d778059aaae"}},"metadata":{}},{"name":"stdout","text":"Epoch 3, step 273: Generator loss: 0.7946695685386658, discriminator loss: 0.49262765049934387\nEpoch 3, step 274: Generator loss: 0.7948476076126099, discriminator loss: 0.4925040602684021\nEpoch 3, step 275: Generator loss: 0.7950246334075928, discriminator loss: 0.4900907576084137\nEpoch 3, step 276: Generator loss: 0.7952019572257996, discriminator loss: 0.4895666539669037\nEpoch 3, step 277: Generator loss: 0.7953716516494751, discriminator loss: 0.49159368872642517\nEpoch 3, step 278: Generator loss: 0.795540452003479, discriminator loss: 0.48839259147644043\nEpoch 3, step 279: Generator loss: 0.7957139611244202, discriminator loss: 0.4857032597064972\nEpoch 3, step 280: Generator loss: 0.795890212059021, discriminator loss: 0.48727506399154663\nEpoch 3, step 281: Generator loss: 0.796064019203186, discriminator loss: 0.4865760803222656\nEpoch 3, step 282: Generator loss: 0.7962403297424316, discriminator loss: 0.48473048210144043\nEpoch 3, step 283: Generator loss: 0.7964183688163757, discriminator loss: 0.4835873544216156\nEpoch 3, step 284: Generator loss: 0.7965911626815796, discriminator loss: 0.4839710593223572\nEpoch 3, step 285: Generator loss: 0.7967644929885864, discriminator loss: 0.4843975305557251\nEpoch 3, step 286: Generator loss: 0.7969363927841187, discriminator loss: 0.48239266872406006\nEpoch 3, step 287: Generator loss: 0.7971084117889404, discriminator loss: 0.4806578457355499\nEpoch 3, step 288: Generator loss: 0.7972754240036011, discriminator loss: 0.48257601261138916\nEpoch 3, step 289: Generator loss: 0.797433078289032, discriminator loss: 0.480409175157547\nEpoch 3, step 290: Generator loss: 0.7975913882255554, discriminator loss: 0.47836533188819885\nEpoch 3, step 291: Generator loss: 0.7977425456047058, discriminator loss: 0.4796288311481476\nEpoch 3, step 292: Generator loss: 0.7978929877281189, discriminator loss: 0.4777737855911255\nEpoch 3, step 293: Generator loss: 0.7980422973632812, discriminator loss: 0.47741395235061646\nEpoch 3, step 294: Generator loss: 0.7981879115104675, discriminator loss: 0.47739848494529724\nEpoch 3, step 295: Generator loss: 0.7983285188674927, discriminator loss: 0.47777968645095825\nEpoch 3, step 296: Generator loss: 0.7984627485275269, discriminator loss: 0.4761401414871216\nEpoch 3, step 297: Generator loss: 0.7985986471176147, discriminator loss: 0.47394874691963196\nEpoch 3, step 298: Generator loss: 0.7987374663352966, discriminator loss: 0.47388356924057007\nEpoch 3, step 299: Generator loss: 0.7988773584365845, discriminator loss: 0.4728434681892395\nEpoch 3, step 300: Generator loss: 0.7990179061889648, discriminator loss: 0.4737933278083801\nEpoch 3, step 301: Generator loss: 0.7991580963134766, discriminator loss: 0.47056111693382263\nEpoch 3, step 302: Generator loss: 0.7992969155311584, discriminator loss: 0.4713183641433716\nEpoch 3, step 303: Generator loss: 0.7994353175163269, discriminator loss: 0.47025981545448303\nEpoch 3, step 304: Generator loss: 0.7995675206184387, discriminator loss: 0.47229450941085815\nEpoch 3, step 305: Generator loss: 0.7996984124183655, discriminator loss: 0.4687315821647644\nEpoch 3, step 306: Generator loss: 0.7998263835906982, discriminator loss: 0.46902668476104736\nEpoch 3, step 307: Generator loss: 0.799954891204834, discriminator loss: 0.4692355990409851\nEpoch 3, step 308: Generator loss: 0.8000826239585876, discriminator loss: 0.46596699953079224\nEpoch 3, step 309: Generator loss: 0.8002080917358398, discriminator loss: 0.46659934520721436\nEpoch 3, step 310: Generator loss: 0.8003373742103577, discriminator loss: 0.4651571214199066\nEpoch 3, step 311: Generator loss: 0.8004637360572815, discriminator loss: 0.4642496407032013\nEpoch 3, step 312: Generator loss: 0.8005902171134949, discriminator loss: 0.46378204226493835\nEpoch 3, step 313: Generator loss: 0.8007144927978516, discriminator loss: 0.4636581242084503\nEpoch 3, step 314: Generator loss: 0.8008367419242859, discriminator loss: 0.46184635162353516\nEpoch 3, step 315: Generator loss: 0.8009621500968933, discriminator loss: 0.46309906244277954\nEpoch 3, step 316: Generator loss: 0.8010867834091187, discriminator loss: 0.461161345243454\nEpoch 3, step 317: Generator loss: 0.8012123107910156, discriminator loss: 0.46515268087387085\nEpoch 3, step 318: Generator loss: 0.8013398051261902, discriminator loss: 0.45958447456359863\nEpoch 3, step 319: Generator loss: 0.8014659881591797, discriminator loss: 0.46148377656936646\nEpoch 3, step 320: Generator loss: 0.8015964031219482, discriminator loss: 0.45871245861053467\nEpoch 3, step 321: Generator loss: 0.8017268180847168, discriminator loss: 0.45921969413757324\nEpoch 3, step 322: Generator loss: 0.8018600940704346, discriminator loss: 0.4578087329864502\nEpoch 3, step 323: Generator loss: 0.8019950985908508, discriminator loss: 0.4562217891216278\nEpoch 3, step 324: Generator loss: 0.8021304607391357, discriminator loss: 0.45722541213035583\nEpoch 3, step 325: Generator loss: 0.8022693991661072, discriminator loss: 0.454551637172699\nEpoch 3, step 326: Generator loss: 0.8024150133132935, discriminator loss: 0.4539794623851776\nEpoch 3, step 327: Generator loss: 0.8025605082511902, discriminator loss: 0.4541420340538025\nEpoch 3, step 328: Generator loss: 0.8027093410491943, discriminator loss: 0.4526670277118683\nEpoch 3, step 329: Generator loss: 0.8028600811958313, discriminator loss: 0.4533141255378723\nEpoch 3, step 330: Generator loss: 0.8030096888542175, discriminator loss: 0.45402058959007263\nEpoch 3, step 331: Generator loss: 0.8031629920005798, discriminator loss: 0.45154422521591187\nEpoch 3, step 332: Generator loss: 0.8033216595649719, discriminator loss: 0.4491662383079529\nEpoch 3, step 333: Generator loss: 0.8034833073616028, discriminator loss: 0.44970598816871643\nEpoch 3, step 334: Generator loss: 0.8036428093910217, discriminator loss: 0.4509289860725403\nEpoch 3, step 335: Generator loss: 0.8038030862808228, discriminator loss: 0.44965702295303345\nEpoch 3, step 336: Generator loss: 0.8039639592170715, discriminator loss: 0.4483349621295929\nEpoch 3, step 337: Generator loss: 0.8041220307350159, discriminator loss: 0.4488043189048767\nEpoch 3, step 338: Generator loss: 0.8042812943458557, discriminator loss: 0.44537150859832764\nEpoch 3, step 339: Generator loss: 0.8044413924217224, discriminator loss: 0.4481191039085388\nEpoch 3, step 340: Generator loss: 0.8046023845672607, discriminator loss: 0.4466888904571533\nEpoch 3, step 341: Generator loss: 0.8047657608985901, discriminator loss: 0.444745272397995\nEpoch 3, step 342: Generator loss: 0.804931640625, discriminator loss: 0.44507184624671936\nEpoch 3, step 343: Generator loss: 0.8051031231880188, discriminator loss: 0.44269758462905884\nEpoch 3, step 344: Generator loss: 0.8052732348442078, discriminator loss: 0.4443703889846802\nEpoch 3, step 345: Generator loss: 0.8054483532905579, discriminator loss: 0.44010794162750244\nEpoch 3, step 346: Generator loss: 0.8056277632713318, discriminator loss: 0.44253262877464294\nEpoch 3, step 347: Generator loss: 0.8058122396469116, discriminator loss: 0.44079113006591797\nEpoch 3, step 348: Generator loss: 0.8060010075569153, discriminator loss: 0.4385661482810974\nEpoch 3, step 349: Generator loss: 0.8061937689781189, discriminator loss: 0.43770137429237366\nEpoch 3, step 350: Generator loss: 0.806389570236206, discriminator loss: 0.43772652745246887\nEpoch 3, step 351: Generator loss: 0.8065853118896484, discriminator loss: 0.4391363859176636\nEpoch 3, step 352: Generator loss: 0.8067810535430908, discriminator loss: 0.43898439407348633\nEpoch 3, step 353: Generator loss: 0.8069747686386108, discriminator loss: 0.4380851686000824\nEpoch 3, step 354: Generator loss: 0.8071694374084473, discriminator loss: 0.4370328187942505\nEpoch 3, step 355: Generator loss: 0.8073723912239075, discriminator loss: 0.4324476420879364\nEpoch 3, step 356: Generator loss: 0.8075756430625916, discriminator loss: 0.4362964332103729\nEpoch 3, step 357: Generator loss: 0.80777907371521, discriminator loss: 0.4345031678676605\nEpoch 3, step 358: Generator loss: 0.8079873323440552, discriminator loss: 0.43053197860717773\nEpoch 3, step 359: Generator loss: 0.8081978559494019, discriminator loss: 0.4330842196941376\nEpoch 3, step 360: Generator loss: 0.8084079027175903, discriminator loss: 0.43174654245376587\nEpoch 3, step 361: Generator loss: 0.8086208701133728, discriminator loss: 0.43043649196624756\nEpoch 3, step 362: Generator loss: 0.8088288903236389, discriminator loss: 0.4313651919364929\nEpoch 3, step 363: Generator loss: 0.8090252876281738, discriminator loss: 0.4352198839187622\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/91 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2fed61f4d93841668aad7fdc9811bae0"}},"metadata":{}},{"name":"stdout","text":"Epoch 4, step 364: Generator loss: 0.8092284798622131, discriminator loss: 0.4286654591560364\nEpoch 4, step 365: Generator loss: 0.8094350695610046, discriminator loss: 0.42920517921447754\nEpoch 4, step 366: Generator loss: 0.8096445202827454, discriminator loss: 0.4287486672401428\nEpoch 4, step 367: Generator loss: 0.809852659702301, discriminator loss: 0.42910730838775635\nEpoch 4, step 368: Generator loss: 0.8100660443305969, discriminator loss: 0.4257209300994873\nEpoch 4, step 369: Generator loss: 0.8102830052375793, discriminator loss: 0.4258714020252228\nEpoch 4, step 370: Generator loss: 0.8104983568191528, discriminator loss: 0.4259628355503082\nEpoch 4, step 371: Generator loss: 0.8107077479362488, discriminator loss: 0.4276059567928314\nEpoch 4, step 372: Generator loss: 0.8109186887741089, discriminator loss: 0.4253161549568176\nEpoch 4, step 373: Generator loss: 0.8111311197280884, discriminator loss: 0.4232766926288605\nEpoch 4, step 374: Generator loss: 0.8113473057746887, discriminator loss: 0.4245963990688324\nEpoch 4, step 375: Generator loss: 0.8115701079368591, discriminator loss: 0.4225642681121826\nEpoch 4, step 376: Generator loss: 0.8117971420288086, discriminator loss: 0.42160913348197937\nEpoch 4, step 377: Generator loss: 0.8120200037956238, discriminator loss: 0.4224117398262024\nEpoch 4, step 378: Generator loss: 0.8122474551200867, discriminator loss: 0.41993582248687744\nEpoch 4, step 379: Generator loss: 0.812475860118866, discriminator loss: 0.4208637773990631\nEpoch 4, step 380: Generator loss: 0.812706708908081, discriminator loss: 0.41999393701553345\nEpoch 4, step 381: Generator loss: 0.8129400014877319, discriminator loss: 0.41930460929870605\nEpoch 4, step 382: Generator loss: 0.813167929649353, discriminator loss: 0.42096880078315735\nEpoch 4, step 383: Generator loss: 0.8133994936943054, discriminator loss: 0.41807568073272705\nEpoch 4, step 384: Generator loss: 0.8136313557624817, discriminator loss: 0.41954582929611206\nEpoch 4, step 385: Generator loss: 0.813865065574646, discriminator loss: 0.41769903898239136\nEpoch 4, step 386: Generator loss: 0.8141000270843506, discriminator loss: 0.4161882996559143\nEpoch 4, step 387: Generator loss: 0.8143380284309387, discriminator loss: 0.4154251515865326\nEpoch 4, step 388: Generator loss: 0.8145768642425537, discriminator loss: 0.41549089550971985\nEpoch 4, step 389: Generator loss: 0.8148126602172852, discriminator loss: 0.4170515239238739\nEpoch 4, step 390: Generator loss: 0.815048098564148, discriminator loss: 0.41447898745536804\nEpoch 4, step 391: Generator loss: 0.81528240442276, discriminator loss: 0.41374561190605164\nEpoch 4, step 392: Generator loss: 0.8155149221420288, discriminator loss: 0.41265326738357544\nEpoch 4, step 393: Generator loss: 0.8157512545585632, discriminator loss: 0.4132649302482605\nEpoch 4, step 394: Generator loss: 0.815994143486023, discriminator loss: 0.4107411205768585\nEpoch 4, step 395: Generator loss: 0.8162323236465454, discriminator loss: 0.4136936366558075\nEpoch 4, step 396: Generator loss: 0.8164767622947693, discriminator loss: 0.4087618589401245\nEpoch 4, step 397: Generator loss: 0.8167256116867065, discriminator loss: 0.4108589291572571\nEpoch 4, step 398: Generator loss: 0.8169753551483154, discriminator loss: 0.40935027599334717\nEpoch 4, step 399: Generator loss: 0.817229151725769, discriminator loss: 0.4089624881744385\nEpoch 4, step 400: Generator loss: 0.8174846172332764, discriminator loss: 0.410076767206192\nEpoch 4, step 401: Generator loss: 0.8177412152290344, discriminator loss: 0.40809398889541626\nEpoch 4, step 402: Generator loss: 0.8180009126663208, discriminator loss: 0.40903240442276\nEpoch 4, step 403: Generator loss: 0.8182634115219116, discriminator loss: 0.40525761246681213\nEpoch 4, step 404: Generator loss: 0.8185237646102905, discriminator loss: 0.40723270177841187\nEpoch 4, step 405: Generator loss: 0.8187849521636963, discriminator loss: 0.4055466055870056\nEpoch 4, step 406: Generator loss: 0.819037914276123, discriminator loss: 0.40835562348365784\nEpoch 4, step 407: Generator loss: 0.8192899227142334, discriminator loss: 0.40542036294937134\nEpoch 4, step 408: Generator loss: 0.819547712802887, discriminator loss: 0.4037354588508606\nEpoch 4, step 409: Generator loss: 0.8198067545890808, discriminator loss: 0.40481868386268616\nEpoch 4, step 410: Generator loss: 0.8200648427009583, discriminator loss: 0.4029484987258911\nEpoch 4, step 411: Generator loss: 0.8203256130218506, discriminator loss: 0.40254640579223633\nEpoch 4, step 412: Generator loss: 0.8205834627151489, discriminator loss: 0.40274345874786377\nEpoch 4, step 413: Generator loss: 0.8208457827568054, discriminator loss: 0.400470495223999\nEpoch 4, step 414: Generator loss: 0.8211098313331604, discriminator loss: 0.4019700586795807\nEpoch 4, step 415: Generator loss: 0.8213791251182556, discriminator loss: 0.39924952387809753\nEpoch 4, step 416: Generator loss: 0.8216480612754822, discriminator loss: 0.39929044246673584\nEpoch 4, step 417: Generator loss: 0.8219143152236938, discriminator loss: 0.4017896056175232\nEpoch 4, step 418: Generator loss: 0.8221793174743652, discriminator loss: 0.39971017837524414\nEpoch 4, step 419: Generator loss: 0.8224459886550903, discriminator loss: 0.397757351398468\nEpoch 4, step 420: Generator loss: 0.8227130174636841, discriminator loss: 0.39793601632118225\nEpoch 4, step 421: Generator loss: 0.8229854106903076, discriminator loss: 0.3962744474411011\nEpoch 4, step 422: Generator loss: 0.8232588768005371, discriminator loss: 0.39811956882476807\nEpoch 4, step 423: Generator loss: 0.8235366344451904, discriminator loss: 0.3963518440723419\nEpoch 4, step 424: Generator loss: 0.823817789554596, discriminator loss: 0.39587944746017456\nEpoch 4, step 425: Generator loss: 0.8241003155708313, discriminator loss: 0.39503008127212524\nEpoch 4, step 426: Generator loss: 0.8243839144706726, discriminator loss: 0.3950863480567932\nEpoch 4, step 427: Generator loss: 0.8246655464172363, discriminator loss: 0.3943973183631897\nEpoch 4, step 428: Generator loss: 0.8249462842941284, discriminator loss: 0.39287251234054565\nEpoch 4, step 429: Generator loss: 0.8252283930778503, discriminator loss: 0.3941102623939514\nEpoch 4, step 430: Generator loss: 0.8255146741867065, discriminator loss: 0.3912382125854492\nEpoch 4, step 431: Generator loss: 0.8258010745048523, discriminator loss: 0.39179718494415283\nEpoch 4, step 432: Generator loss: 0.826081395149231, discriminator loss: 0.39329928159713745\nEpoch 4, step 433: Generator loss: 0.8263591527938843, discriminator loss: 0.3917464017868042\nEpoch 4, step 434: Generator loss: 0.8266422152519226, discriminator loss: 0.38994768261909485\nEpoch 4, step 435: Generator loss: 0.8269258737564087, discriminator loss: 0.3907710313796997\nEpoch 4, step 436: Generator loss: 0.8272048830986023, discriminator loss: 0.39230877161026\nEpoch 4, step 437: Generator loss: 0.8274850249290466, discriminator loss: 0.38882777094841003\nEpoch 4, step 438: Generator loss: 0.8277685642242432, discriminator loss: 0.3871943950653076\nEpoch 4, step 439: Generator loss: 0.8280550837516785, discriminator loss: 0.38669347763061523\nEpoch 4, step 440: Generator loss: 0.8283424973487854, discriminator loss: 0.3867143988609314\nEpoch 4, step 441: Generator loss: 0.8286288380622864, discriminator loss: 0.38658833503723145\nEpoch 4, step 442: Generator loss: 0.8289106488227844, discriminator loss: 0.3885171413421631\nEpoch 4, step 443: Generator loss: 0.8291954398155212, discriminator loss: 0.3855394721031189\nEpoch 4, step 444: Generator loss: 0.8294785022735596, discriminator loss: 0.3865084946155548\nEpoch 4, step 445: Generator loss: 0.8297637701034546, discriminator loss: 0.385151207447052\nEpoch 4, step 446: Generator loss: 0.8300471305847168, discriminator loss: 0.38608258962631226\nEpoch 4, step 447: Generator loss: 0.8303334712982178, discriminator loss: 0.3841514587402344\nEpoch 4, step 448: Generator loss: 0.8306118249893188, discriminator loss: 0.3860996663570404\nEpoch 4, step 449: Generator loss: 0.830889105796814, discriminator loss: 0.3841524124145508\nEpoch 4, step 450: Generator loss: 0.8311730623245239, discriminator loss: 0.38101235032081604\nEpoch 4, step 451: Generator loss: 0.831457793712616, discriminator loss: 0.3814944922924042\nEpoch 4, step 452: Generator loss: 0.8317412734031677, discriminator loss: 0.3819359838962555\nEpoch 4, step 453: Generator loss: 0.83202064037323, discriminator loss: 0.3825839161872864\nEpoch 4, step 454: Generator loss: 0.8323116898536682, discriminator loss: 0.3776654303073883\n","output_type":"stream"}]},{"cell_type":"code","source":"res=gen(X_oversampled.float().to(device))","metadata":{"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"res","metadata":{"trusted":true},"execution_count":65,"outputs":[{"execution_count":65,"output_type":"execute_result","data":{"text/plain":"tensor([[0.4230, 0.0877, 0.0752,  ..., 0.0628, 0.0542, 0.0784],\n        [0.0411, 0.2723, 0.0879,  ..., 0.0827, 0.0447, 0.0472],\n        [0.0514, 0.0936, 0.0876,  ..., 0.1262, 0.0725, 0.1492],\n        ...,\n        [0.0233, 0.0841, 0.0541,  ..., 0.0983, 0.0680, 0.1080],\n        [0.0462, 0.0811, 0.0617,  ..., 0.1158, 0.0750, 0.1395],\n        [0.1261, 0.1657, 0.1431,  ..., 0.0952, 0.1298, 0.0925]],\n       device='cuda:0', grad_fn=<SigmoidBackward>)"},"metadata":{}}]},{"cell_type":"code","source":"X_oversampled","metadata":{"trusted":true},"execution_count":66,"outputs":[{"execution_count":66,"output_type":"execute_result","data":{"text/plain":"tensor([[ 0.0661,  1.0000, -1.0000,  ...,  1.0000,  1.0000,  1.0000],\n        [ 1.0000,  1.0000, -1.0000,  ...,  1.0000, -1.0000, -1.0000],\n        [ 1.0000, -1.0000,  1.0000,  ..., -0.0511, -1.0000,  1.0000],\n        ...,\n        [-1.0000, -1.0000,  1.0000,  ...,  1.0000, -1.0000, -1.0000],\n        [ 1.0000, -0.6532,  0.6532,  ..., -1.0000, -1.0000,  1.0000],\n        [ 1.0000,  1.0000,  1.0000,  ..., -1.0000, -1.0000,  1.0000]],\n       dtype=torch.float64)"},"metadata":{}}]},{"cell_type":"code","source":"fres=res.cpu().detach().numpy()\nfres.shape","metadata":{"trusted":true},"execution_count":67,"outputs":[{"execution_count":67,"output_type":"execute_result","data":{"text/plain":"(278188, 42)"},"metadata":{}}]},{"cell_type":"code","source":"fin=np.concatenate((X_train_res[:301312], fres), axis=0)","metadata":{"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"def shuffle_in_unison(a, b):\n    assert len(a) == len(b)\n    shuffled_a = np.empty(a.shape, dtype=a.dtype)\n    shuffled_b = np.empty(b.shape, dtype=b.dtype)\n    permutation = np.random.permutation(len(a))\n    for old_index, new_index in enumerate(permutation):\n        shuffled_a[new_index] = a[old_index]\n        shuffled_b[new_index] = b[old_index]\n    return shuffled_a, shuffled_b","metadata":{"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"y_train_res.shape","metadata":{"trusted":true},"execution_count":70,"outputs":[{"execution_count":70,"output_type":"execute_result","data":{"text/plain":"(579500,)"},"metadata":{}}]},{"cell_type":"code","source":"Xn,yn=shuffle_in_unison(fin, y_train_res)\n","metadata":{"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"callf1(Xn, yn.ravel(),X_test,  y_test.ravel())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ste3=[]\nsta3=[]\nsf3=[]\nfor i in range(30):\n    r=callf1(Xn, yn.ravel(),X_test,  y_test.ravel())\n    ste3.append(r[0])\n    sta3.append(r[1])\n    sf3.append(r[2])","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":72,"outputs":[{"name":"stdout","text":"Epoch 1/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0301 - accuracy: 0.9811\nEpoch 2/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0087 - accuracy: 0.9960\nEpoch 3/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0063 - accuracy: 0.9979\n2354/2354 - 2s - loss: 0.0104 - accuracy: 0.9963\n\nTest accuracy: 0.9963492751121521\n18110/18110 - 24s - loss: 0.0056 - accuracy: 0.9986\nEpoch 1/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0312 - accuracy: 0.9806\nEpoch 2/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0090 - accuracy: 0.9957\nEpoch 3/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0066 - accuracy: 0.9977\n2354/2354 - 2s - loss: 0.0099 - accuracy: 0.9962\n\nTest accuracy: 0.9962431192398071\n18110/18110 - 24s - loss: 0.0046 - accuracy: 0.9986\nEpoch 1/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0329 - accuracy: 0.9801\nEpoch 2/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0088 - accuracy: 0.9958\nEpoch 3/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0065 - accuracy: 0.9979\n2354/2354 - 2s - loss: 0.0101 - accuracy: 0.9958\n\nTest accuracy: 0.9958183169364929\n18110/18110 - 24s - loss: 0.0055 - accuracy: 0.9985\nEpoch 1/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0324 - accuracy: 0.9805\nEpoch 2/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0087 - accuracy: 0.9963\nEpoch 3/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0067 - accuracy: 0.9981\n2354/2354 - 2s - loss: 0.0110 - accuracy: 0.9964\n\nTest accuracy: 0.9964289665222168\n18110/18110 - 24s - loss: 0.0067 - accuracy: 0.9987\nEpoch 1/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0334 - accuracy: 0.9787\nEpoch 2/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0122 - accuracy: 0.9914\nEpoch 3/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0103 - accuracy: 0.9930\n2354/2354 - 2s - loss: 0.0186 - accuracy: 0.9858\n\nTest accuracy: 0.9857954382896423\n18110/18110 - 24s - loss: 0.0099 - accuracy: 0.9931\nEpoch 1/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0315 - accuracy: 0.9809\nEpoch 2/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0091 - accuracy: 0.9955\nEpoch 3/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0069 - accuracy: 0.9979\n2354/2354 - 2s - loss: 0.0106 - accuracy: 0.9967\n\nTest accuracy: 0.9966811537742615\n18110/18110 - 24s - loss: 0.0060 - accuracy: 0.9987\nEpoch 1/3\n18110/18110 [==============================] - 37s 2ms/step - loss: 0.0317 - accuracy: 0.9803\nEpoch 2/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0096 - accuracy: 0.9950\nEpoch 3/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0071 - accuracy: 0.9975\n2354/2354 - 2s - loss: 0.0111 - accuracy: 0.9957\n\nTest accuracy: 0.9957253336906433\n18110/18110 - 24s - loss: 0.0065 - accuracy: 0.9982\nEpoch 1/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0311 - accuracy: 0.9804\nEpoch 2/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0111 - accuracy: 0.9926\nEpoch 3/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0093 - accuracy: 0.9941\n2354/2354 - 2s - loss: 0.0159 - accuracy: 0.9885\n\nTest accuracy: 0.9884505271911621\n18110/18110 - 24s - loss: 0.0079 - accuracy: 0.9944\nEpoch 1/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0315 - accuracy: 0.9802\nEpoch 2/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0100 - accuracy: 0.9947\nEpoch 3/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0072 - accuracy: 0.9973\n2354/2354 - 2s - loss: 0.0105 - accuracy: 0.9964\n\nTest accuracy: 0.996362566947937\n18110/18110 - 24s - loss: 0.0062 - accuracy: 0.9986\nEpoch 1/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0317 - accuracy: 0.9803\nEpoch 2/3\n18110/18110 [==============================] - 37s 2ms/step - loss: 0.0083 - accuracy: 0.9962\nEpoch 3/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0067 - accuracy: 0.9980\n2354/2354 - 2s - loss: 0.0110 - accuracy: 0.9968\n\nTest accuracy: 0.9968006610870361\n18110/18110 - 24s - loss: 0.0060 - accuracy: 0.9987\nEpoch 1/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0332 - accuracy: 0.9799\nEpoch 2/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0093 - accuracy: 0.9954\nEpoch 3/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0065 - accuracy: 0.9977\n2354/2354 - 2s - loss: 0.0105 - accuracy: 0.9956\n\nTest accuracy: 0.9955793619155884\n18110/18110 - 24s - loss: 0.0088 - accuracy: 0.9983\nEpoch 1/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0329 - accuracy: 0.9795\nEpoch 2/3\n18110/18110 [==============================] - 37s 2ms/step - loss: 0.0149 - accuracy: 0.9879\nEpoch 3/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0134 - accuracy: 0.9889\n2354/2354 - 2s - loss: 0.0247 - accuracy: 0.9782\n\nTest accuracy: 0.9782019853591919\n18110/18110 - 24s - loss: 0.0130 - accuracy: 0.9893\nEpoch 1/3\n18110/18110 [==============================] - 37s 2ms/step - loss: 0.0312 - accuracy: 0.9807\nEpoch 2/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0086 - accuracy: 0.9959\nEpoch 3/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0067 - accuracy: 0.9979\n2354/2354 - 2s - loss: 0.0108 - accuracy: 0.9964\n\nTest accuracy: 0.9964156746864319\n18110/18110 - 24s - loss: 0.0059 - accuracy: 0.9987\nEpoch 1/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0315 - accuracy: 0.9806\nEpoch 2/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0089 - accuracy: 0.9958\nEpoch 3/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0065 - accuracy: 0.9978\n2354/2354 - 2s - loss: 0.0097 - accuracy: 0.9963\n\nTest accuracy: 0.996336042881012\n18110/18110 - 24s - loss: 0.0054 - accuracy: 0.9986\nEpoch 1/3\n18110/18110 [==============================] - 37s 2ms/step - loss: 0.0322 - accuracy: 0.9806\nEpoch 2/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0085 - accuracy: 0.9961\nEpoch 3/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0066 - accuracy: 0.9979\n2354/2354 - 2s - loss: 0.0110 - accuracy: 0.9962\n\nTest accuracy: 0.9962431192398071\n18110/18110 - 24s - loss: 0.0054 - accuracy: 0.9985\nEpoch 1/3\n18110/18110 [==============================] - 37s 2ms/step - loss: 0.0331 - accuracy: 0.9786\nEpoch 2/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0090 - accuracy: 0.9958\nEpoch 3/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0065 - accuracy: 0.9980\n2354/2354 - 2s - loss: 0.0098 - accuracy: 0.9966\n\nTest accuracy: 0.9966015219688416\n18110/18110 - 24s - loss: 0.0046 - accuracy: 0.9988\nEpoch 1/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0327 - accuracy: 0.9794\nEpoch 2/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0153 - accuracy: 0.9874\nEpoch 3/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0142 - accuracy: 0.9884\n2354/2354 - 2s - loss: 0.0254 - accuracy: 0.9778\n\nTest accuracy: 0.9778169989585876\n18110/18110 - 24s - loss: 0.0127 - accuracy: 0.9891\nEpoch 1/3\n18110/18110 [==============================] - 37s 2ms/step - loss: 0.0326 - accuracy: 0.9791\nEpoch 2/3\n18110/18110 [==============================] - 37s 2ms/step - loss: 0.0149 - accuracy: 0.9878\nEpoch 3/3\n18110/18110 [==============================] - 37s 2ms/step - loss: 0.0137 - accuracy: 0.9887\n2354/2354 - 2s - loss: 0.0248 - accuracy: 0.9784\n\nTest accuracy: 0.9784409403800964\n18110/18110 - 24s - loss: 0.0134 - accuracy: 0.9892\nEpoch 1/3\n18110/18110 [==============================] - 37s 2ms/step - loss: 0.0341 - accuracy: 0.9794\nEpoch 2/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0158 - accuracy: 0.9873\nEpoch 3/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0138 - accuracy: 0.9888\n2354/2354 - 2s - loss: 0.0247 - accuracy: 0.9783\n\nTest accuracy: 0.9782816767692566\n18110/18110 - 24s - loss: 0.0125 - accuracy: 0.9893\nEpoch 1/3\n18110/18110 [==============================] - 37s 2ms/step - loss: 0.0307 - accuracy: 0.9804\nEpoch 2/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0096 - accuracy: 0.9951\nEpoch 3/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0067 - accuracy: 0.9980\n2354/2354 - 2s - loss: 0.0110 - accuracy: 0.9956\n\nTest accuracy: 0.9956457018852234\n18110/18110 - 25s - loss: 0.0056 - accuracy: 0.9985\nEpoch 1/3\n18110/18110 [==============================] - 37s 2ms/step - loss: 0.0318 - accuracy: 0.9804\nEpoch 2/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0113 - accuracy: 0.9925\nEpoch 3/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0101 - accuracy: 0.9936\n2354/2354 - 2s - loss: 0.0181 - accuracy: 0.9874\n\nTest accuracy: 0.9873884916305542\n18110/18110 - 24s - loss: 0.0100 - accuracy: 0.9943\nEpoch 1/3\n18110/18110 [==============================] - 37s 2ms/step - loss: 0.0330 - accuracy: 0.9788\nEpoch 2/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0153 - accuracy: 0.9877\nEpoch 3/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0135 - accuracy: 0.9889\n2354/2354 - 2s - loss: 0.0243 - accuracy: 0.9783\n\nTest accuracy: 0.9783214926719666\n18110/18110 - 24s - loss: 0.0139 - accuracy: 0.9893\nEpoch 1/3\n18110/18110 [==============================] - 37s 2ms/step - loss: 0.0320 - accuracy: 0.9797\nEpoch 2/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0147 - accuracy: 0.9880\nEpoch 3/3\n18110/18110 [==============================] - 37s 2ms/step - loss: 0.0134 - accuracy: 0.9890\n2354/2354 - 2s - loss: 0.0239 - accuracy: 0.9786\n\nTest accuracy: 0.978613555431366\n18110/18110 - 24s - loss: 0.0135 - accuracy: 0.9894\nEpoch 1/3\n18110/18110 [==============================] - 37s 2ms/step - loss: 0.0325 - accuracy: 0.9794\nEpoch 2/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0147 - accuracy: 0.9882\nEpoch 3/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0131 - accuracy: 0.9892\n2354/2354 - 2s - loss: 0.0242 - accuracy: 0.9781\n\nTest accuracy: 0.9780559539794922\n18110/18110 - 24s - loss: 0.0122 - accuracy: 0.9893\nEpoch 1/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0333 - accuracy: 0.9787\nEpoch 2/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0118 - accuracy: 0.9921\nEpoch 3/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0096 - accuracy: 0.9939\n2354/2354 - 2s - loss: 0.0175 - accuracy: 0.9882\n\nTest accuracy: 0.9881982803344727\n18110/18110 - 24s - loss: 0.0099 - accuracy: 0.9943\nEpoch 1/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0323 - accuracy: 0.9795\nEpoch 2/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0101 - accuracy: 0.9947\nEpoch 3/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0074 - accuracy: 0.9973\n2354/2354 - 2s - loss: 0.0120 - accuracy: 0.9949\n\nTest accuracy: 0.9949155449867249\n18110/18110 - 24s - loss: 0.0060 - accuracy: 0.9979\nEpoch 1/3\n18110/18110 [==============================] - 37s 2ms/step - loss: 0.0328 - accuracy: 0.9792\nEpoch 2/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0149 - accuracy: 0.9881\nEpoch 3/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0135 - accuracy: 0.9891\n2354/2354 - 2s - loss: 0.0245 - accuracy: 0.9785\n\nTest accuracy: 0.9784542322158813\n18110/18110 - 24s - loss: 0.0129 - accuracy: 0.9893\nEpoch 1/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0335 - accuracy: 0.9796\nEpoch 2/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0144 - accuracy: 0.9884\nEpoch 3/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0132 - accuracy: 0.9891\n2354/2354 - 2s - loss: 0.0232 - accuracy: 0.9788\n\nTest accuracy: 0.9788392186164856\n18110/18110 - 24s - loss: 0.0122 - accuracy: 0.9894\nEpoch 1/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0320 - accuracy: 0.9800\nEpoch 2/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0099 - accuracy: 0.9941\nEpoch 3/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0074 - accuracy: 0.9974\n2354/2354 - 2s - loss: 0.0116 - accuracy: 0.9958\n\nTest accuracy: 0.9958183169364929\n18110/18110 - 25s - loss: 0.0058 - accuracy: 0.9984\nEpoch 1/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0324 - accuracy: 0.9793\nEpoch 2/3\n18110/18110 [==============================] - 36s 2ms/step - loss: 0.0155 - accuracy: 0.9873\nEpoch 3/3\n18110/18110 [==============================] - 37s 2ms/step - loss: 0.0141 - accuracy: 0.9887\n2354/2354 - 2s - loss: 0.0259 - accuracy: 0.9772\n\nTest accuracy: 0.9772196412086487\n18110/18110 - 24s - loss: 0.0135 - accuracy: 0.9888\n","output_type":"stream"}]},{"cell_type":"code","source":"sum(sf3)/30","metadata":{"trusted":true},"execution_count":116,"outputs":[{"execution_count":116,"output_type":"execute_result","data":{"text/plain":"0.9034147136960956"},"metadata":{}}]},{"cell_type":"code","source":"sum(sta3)/30","metadata":{"trusted":true},"execution_count":117,"outputs":[{"execution_count":117,"output_type":"execute_result","data":{"text/plain":"0.9948261717955271"},"metadata":{}}]},{"cell_type":"code","source":"sum(ste3)/30","metadata":{"trusted":true},"execution_count":118,"outputs":[{"execution_count":118,"output_type":"execute_result","data":{"text/plain":"0.989001437028249"},"metadata":{}}]},{"cell_type":"code","source":"stdev(sf3)","metadata":{"trusted":true},"execution_count":119,"outputs":[{"execution_count":119,"output_type":"execute_result","data":{"text/plain":"0.07886478248997422"},"metadata":{}}]},{"cell_type":"code","source":"stdev(sta3)","metadata":{"trusted":true},"execution_count":120,"outputs":[{"execution_count":120,"output_type":"execute_result","data":{"text/plain":"0.004293353239982928"},"metadata":{}}]},{"cell_type":"code","source":"stdev(ste3)","metadata":{"trusted":true},"execution_count":121,"outputs":[{"execution_count":121,"output_type":"execute_result","data":{"text/plain":"0.008288316979252124"},"metadata":{}}]},{"cell_type":"code","source":"max(sf3)","metadata":{"trusted":true},"execution_count":122,"outputs":[{"execution_count":122,"output_type":"execute_result","data":{"text/plain":"0.9596652719665273"},"metadata":{}}]},{"cell_type":"code","source":"max(sta3)","metadata":{"trusted":true},"execution_count":123,"outputs":[{"execution_count":123,"output_type":"execute_result","data":{"text/plain":"0.9987661838531494"},"metadata":{}}]},{"cell_type":"code","source":"max(ste3)","metadata":{"trusted":true},"execution_count":124,"outputs":[{"execution_count":124,"output_type":"execute_result","data":{"text/plain":"0.9968006610870361"},"metadata":{}}]},{"cell_type":"code","source":"progress=1","metadata":{"trusted":true},"execution_count":127,"outputs":[]},{"cell_type":"code","source":"save=1","metadata":{"trusted":true},"execution_count":128,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}